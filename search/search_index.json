{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Hi Everyone! I'm Hamzah, an AI/ML engineer and an adjunct professor of Statistics. On this page, you will find a collection of notes spanning topics from probability & statistics to machine learning. Whether you\u2019re just starting out or looking to deepen your understanding, there\u2019s something here for everyone!","title":"Introduction"},{"location":"#introduction","text":"Hi Everyone! I'm Hamzah, an AI/ML engineer and an adjunct professor of Statistics. On this page, you will find a collection of notes spanning topics from probability & statistics to machine learning. Whether you\u2019re just starting out or looking to deepen your understanding, there\u2019s something here for everyone!","title":"Introduction"},{"location":"Statistical%20Inference/Section%201%3A%20statistical_inference/","text":"Statistical Inference Statistical Inference is the process of drawing general conclusions about the population based on insights gathered from a sample. Often, when studying a population, collecting information from every individual is impractical or impossible. Thus, we rely on analyzing smaller samples to make informed inferences on the entire population. The field of Statistics is built on the question: How reliable are the conclusions drawn from a sample? Before we delve into answering the above question, let's go over a few defintions. \\large{\\textbf{Parameter vs Statistic}} parameter: a number that summarizes entire population data. statistic: a number that summarizes sample data. Parameter Statistic Mean \\mu \\overline{x} Variance \\sigma^{2} s^{2} Standard Deviation \\sigma s In other words, we use \\overline{X} to estimate \\mu , s^{2} to estimate \\sigma^{2} , and so on. \\large{\\textbf{Example 1}} Suppose you were interested in the number of hours Rowan students spendy studying per day. You take a random sample of n=100 students and find the average time they spend studying is \\overline{x} = 3.2 hours. Population of interest: Rowan students Parameter of interest: average time all Rowan students spendy studing per day. We use \\overline{x} = 3.2 as our best of \\mu . Note: \\overline{X} is a random variable (unknown until observed) and thus will vary from sample to sample. The randomness in \\overline{X} is in how the sample was obtained. The population parameter, however, is generally thought of as a fixed value. The variability of the sample mean can be studied usings its sampling distribution. The most famous theorem in the field of Statistics is the \\textbf{Central Limit Theorem} which describes the shape of the distribution of \\overline{X} . CLT is tied to normality and under \\textit{certain conditions} , \\overline{X} 's distribution is well approximated by a normal curve. \\large{\\textbf{Central Limit Theorem}} The variability of the sample mean can be studied usings its sampling distribution. The most famous theorem in the field of Statistics is the \\textbf{Central Limit Theorem} which describes the shape of the distribution of \\overline{X} . CLT is tied to normality and under \\textit{certain conditions} , \\overline{X} 's distribution is well approximated by a normal curve. Without going into too much of the theory, the CLT essentially states that \\overline{X} \\sim N(\\mu, \\frac{\\sigma^{2}}{n}) . A general rule of thumb for the CLT to hold is n \\geq 30 . However, it is much more complex than that. In other words, there is no single number that suffices for all applications. If the underlying distribution from which you are sampling is itself normal, then the CLT holds for all sample sizes since the linear combination of indepedent normally distributed random variables is normal. It is reasonable to suggest that the further the population distribution deviates from normality, the larger the sample size required to ensure that the sampling distribution of the mean approaches normality. \\large{\\textbf{Example 2}} Suppose you were interested in estimating the mean number of hours Americans watch tv per week. You, as the investigator, go out and randomly sample n_{1} = 100 americans and find the sample mean to be \\overline{x_{1}} = 10.4 hours. Suppose you repeat this process again. That is to say, you randomly sample n_{2} = 100 americans and find the sample mean to be \\overline{x_{2}} = 9.5 hours. If we were to repeat this process, say 10000 times, we would have 10000 estimates of \\mu from random samples of the population. \\textit{Question: } What is the distribution of these 1000 estimates of \\mu ? Let's assume that \\mu = 10 . We will simulate random samples using the chi-squared distribution. The expected value of a \\chi^{2} distributed random variable is equal to the number of degrees of freedom, which in this case is 10 . The variance is equal to 2*df so in this case \\sigma^{2} = 20 . import numpy as np import matplotlib.pyplot as plt import seaborn as sns import scipy.stats as stat # repeatedly sample from some population, calculate the mean and store it in a list list_of_means = [] iterations = 10000 for j in range(iterations): sample = np.random.chisquare(df=10, size=100) sample_mean = np.mean(sample) list_of_means.append(sample_mean) mean_of_all_means = np.round(np.mean(list_of_means), 3) variance_of_all_means = np.round(np.var(list_of_means), 3) plt.figure(figsize=(10,5)) sns.set_style('darkgrid') sns.histplot(list_of_means, color='green') plt.title(f'Sampling Distribution of Sample Mean') plt.text(10.6, 400, f'Mean of Sample Means: {mean_of_all_means}', fontsize=12) plt.text(10.6, 370, f'Variance of Sample Means: {variance_of_all_means}', fontsize=12) plt.show() In theory, E[\\overline{X}] = 10 and Var[\\overline{X}] = .2 . The results we simulated match with the theory. Naturally, the next question to tackle is how does the sample size affect the sampling distribution of \\overline{X} ? We would expect two things to happen: \\textbf{1} . \\lim_{n\\to\\infty}\\overline{X} = \\mu . In fact, for \\epsilon > 0 , \\displaystyle{\\lim_{n\\to\\infty}} P(|\\overline{X_{n}} - \\mu| < \\epsilon) = 1 That is to say, \\overline{X_{n}} converges in probability to \\mu \\textbf{2} . It is clear that Var[\\overline{X}] = \\frac{\\sigma^{2}}{n} \\to 0 as n \\to \\infty . That is, as we increase our sample isze, we would expect the sampling distribution to be less spread out since there is less uncertainty in our estimates of \\mu . Note: \\sqrt{Var[\\overline{X}]} = \\frac{\\sigma}{\\sqrt{n}} is referred to as the standard error. The term \\textit{standard error} is used because \\overline{X} is treated as an estimator and we want to know the uncertainty or the error in the estimator. \\overline{X} is essentially treated as a surrogate for \\mu Let's investigate this using code! Clearly, the uncertainty in our estimates of \\mu decreases as n increases.","title":"Statistical Inference"},{"location":"Statistical%20Inference/Section%201%3A%20statistical_inference/#statistical-inference","text":"Statistical Inference is the process of drawing general conclusions about the population based on insights gathered from a sample. Often, when studying a population, collecting information from every individual is impractical or impossible. Thus, we rely on analyzing smaller samples to make informed inferences on the entire population. The field of Statistics is built on the question: How reliable are the conclusions drawn from a sample? Before we delve into answering the above question, let's go over a few defintions. \\large{\\textbf{Parameter vs Statistic}} parameter: a number that summarizes entire population data. statistic: a number that summarizes sample data. Parameter Statistic Mean \\mu \\overline{x} Variance \\sigma^{2} s^{2} Standard Deviation \\sigma s In other words, we use \\overline{X} to estimate \\mu , s^{2} to estimate \\sigma^{2} , and so on. \\large{\\textbf{Example 1}} Suppose you were interested in the number of hours Rowan students spendy studying per day. You take a random sample of n=100 students and find the average time they spend studying is \\overline{x} = 3.2 hours. Population of interest: Rowan students Parameter of interest: average time all Rowan students spendy studing per day. We use \\overline{x} = 3.2 as our best of \\mu . Note: \\overline{X} is a random variable (unknown until observed) and thus will vary from sample to sample. The randomness in \\overline{X} is in how the sample was obtained. The population parameter, however, is generally thought of as a fixed value. The variability of the sample mean can be studied usings its sampling distribution. The most famous theorem in the field of Statistics is the \\textbf{Central Limit Theorem} which describes the shape of the distribution of \\overline{X} . CLT is tied to normality and under \\textit{certain conditions} , \\overline{X} 's distribution is well approximated by a normal curve. \\large{\\textbf{Central Limit Theorem}} The variability of the sample mean can be studied usings its sampling distribution. The most famous theorem in the field of Statistics is the \\textbf{Central Limit Theorem} which describes the shape of the distribution of \\overline{X} . CLT is tied to normality and under \\textit{certain conditions} , \\overline{X} 's distribution is well approximated by a normal curve. Without going into too much of the theory, the CLT essentially states that \\overline{X} \\sim N(\\mu, \\frac{\\sigma^{2}}{n}) . A general rule of thumb for the CLT to hold is n \\geq 30 . However, it is much more complex than that. In other words, there is no single number that suffices for all applications. If the underlying distribution from which you are sampling is itself normal, then the CLT holds for all sample sizes since the linear combination of indepedent normally distributed random variables is normal. It is reasonable to suggest that the further the population distribution deviates from normality, the larger the sample size required to ensure that the sampling distribution of the mean approaches normality. \\large{\\textbf{Example 2}} Suppose you were interested in estimating the mean number of hours Americans watch tv per week. You, as the investigator, go out and randomly sample n_{1} = 100 americans and find the sample mean to be \\overline{x_{1}} = 10.4 hours. Suppose you repeat this process again. That is to say, you randomly sample n_{2} = 100 americans and find the sample mean to be \\overline{x_{2}} = 9.5 hours. If we were to repeat this process, say 10000 times, we would have 10000 estimates of \\mu from random samples of the population. \\textit{Question: } What is the distribution of these 1000 estimates of \\mu ? Let's assume that \\mu = 10 . We will simulate random samples using the chi-squared distribution. The expected value of a \\chi^{2} distributed random variable is equal to the number of degrees of freedom, which in this case is 10 . The variance is equal to 2*df so in this case \\sigma^{2} = 20 . import numpy as np import matplotlib.pyplot as plt import seaborn as sns import scipy.stats as stat # repeatedly sample from some population, calculate the mean and store it in a list list_of_means = [] iterations = 10000 for j in range(iterations): sample = np.random.chisquare(df=10, size=100) sample_mean = np.mean(sample) list_of_means.append(sample_mean) mean_of_all_means = np.round(np.mean(list_of_means), 3) variance_of_all_means = np.round(np.var(list_of_means), 3) plt.figure(figsize=(10,5)) sns.set_style('darkgrid') sns.histplot(list_of_means, color='green') plt.title(f'Sampling Distribution of Sample Mean') plt.text(10.6, 400, f'Mean of Sample Means: {mean_of_all_means}', fontsize=12) plt.text(10.6, 370, f'Variance of Sample Means: {variance_of_all_means}', fontsize=12) plt.show() In theory, E[\\overline{X}] = 10 and Var[\\overline{X}] = .2 . The results we simulated match with the theory. Naturally, the next question to tackle is how does the sample size affect the sampling distribution of \\overline{X} ? We would expect two things to happen: \\textbf{1} . \\lim_{n\\to\\infty}\\overline{X} = \\mu . In fact, for \\epsilon > 0 , \\displaystyle{\\lim_{n\\to\\infty}} P(|\\overline{X_{n}} - \\mu| < \\epsilon) = 1 That is to say, \\overline{X_{n}} converges in probability to \\mu \\textbf{2} . It is clear that Var[\\overline{X}] = \\frac{\\sigma^{2}}{n} \\to 0 as n \\to \\infty . That is, as we increase our sample isze, we would expect the sampling distribution to be less spread out since there is less uncertainty in our estimates of \\mu . Note: \\sqrt{Var[\\overline{X}]} = \\frac{\\sigma}{\\sqrt{n}} is referred to as the standard error. The term \\textit{standard error} is used because \\overline{X} is treated as an estimator and we want to know the uncertainty or the error in the estimator. \\overline{X} is essentially treated as a surrogate for \\mu Let's investigate this using code! Clearly, the uncertainty in our estimates of \\mu decreases as n increases.","title":"Statistical Inference"},{"location":"Statistical%20Inference/Section%202%3A%20confidence_intervals/","text":"Confidence Intervals In a previous section , we learned that \\overline{X} is a consistent/trustworthy estimate of \\mu . If n is \"large\", CLT provides that \\bar{X} \\sim N(\\mu, \\frac{\\sigma^{2}}{n}) \\label{clt} \\tag{1} \\textbf{Question} : How can we take advantage of the Central Limit Theorem to learn more about our parameter of interest, \\mu ? Can we construct a range of values that is likely to contain \\mu ? Based on our distribution result, we can standardize \\eqref{clt}: \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1) Here, Z = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} is a standard normal random variable whose distribution does not depend on \\mu or \\sigma . This is referred to as a pivotal quantity. Let P(Z > z_{\\frac{\\alpha}{2}}) = \\frac{\\alpha}{2} and note: \\begin{equation} \\begin{aligned} 1 - \\alpha &= P(-z_{\\frac{\\alpha}{2}} < \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} < z_{\\frac{\\alpha}{2}}) \\\\ & = P(\\overline{X} - z_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}} < \\mu < \\overline{X} + z_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}}) \\\\ \\end{aligned} \\end{equation} \\large{\\textbf{Case I (\u03c3 is known)}} : Let X_{1},...,X_{n} be random sample of size n from a population w/ mean \\mu and \\textit{known} variance \\sigma^{2} . A 100(1-\\alpha)\\% Confidence Interval for \\mu is given by: \\overline{X} \\pm z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} where the value of z_{\\frac{\\alpha}{2}} depends on desired confidence level. Confidence Level Z_{\\frac{\\alpha}{2}} 95% 1.96 90% 1.645 99% 2.576 \\underline{\\textbf{Example}} In a random sample of 75 Rowan students, the mean height was found to be 67 inches. Assuming the population standard deviation is 7 inches, construct a 95% confidence interval for the mean height of all Rowan students. Solution We have \\overline{x} = 67 , \\sigma = 7 , n=75 , z_{\\frac{\\alpha}{2}} = 1.96 . Therefore, a 95% CI for \\mu is: \\begin{equation} \\begin{aligned} & \\hspace{3mm} \\overline{x} \\pm 1.96 \\frac{7}{\\sqrt{75}} \\\\ & = (65.4, 68.58) \\\\ \\end{aligned} \\end{equation} Interpretation: With 95% confidence, we estimate that the mean height of all Rowan students is somewhere between 65.4 and 68.58. Note: from the sample we found \\overline{x} = 67 inches. Using CI, we're saying we're 95% confident that \\mu is somewhere between 65.4 and 68.58. \\textit{Limitation} of Z Confidence Interval: \\sigma is unlikely to be known. \\large{\\textbf{Case I (\u03c3 is unknown)}} : Since \\sigma^{2} in unlikely to be known to the practitioner, we are often limited to s^{2} . The quantity \\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} does not follow a normal distribution. Rather, it follows a t-distribution with n-1 degrees of freedom. For smaller sample sizes, the t-distribution has heavier tails than the normal distribution and thus we expect more extreme values. However, as the degrees of freedom approach infinity, the t-distribution converges to the normal distribution. CI when \u03c3 is unknown Let X_{1}, ..., X_{n} be a random sample from a population with mean \\mu and unknown variance \\sigma^{2} , a 100(1-\\alpha)% CI for \\mu is given by: \\overline{x} \\pm t_{\\frac{\\alpha}{2}, n-1} \\frac{s}{\\sqrt{n}} where t_{\\frac{\\alpha}{2}, n-1} depends on both sample size and confidence level. \\large{\\textbf{CI Coverage}} The correct interpretation of a Confidence Interval is as follows: With 100(1-\\alpha)\\% confidence, we estimate that \\mu is somewhere between L and U. Using the word probability is not correct when it comes to interpreting these intervals. For example, if \\mu = 73 and we calculate a 95\\% confidence interval: (65, 72). There isn't a 95 percent chance that 73 is in the interval! It is a yes/no question. Rather, if we were to calculate 100 different 100(1-\\alpha)\\% confidence intervals for \\mu using 100 different samples of the sample size from the same population, we would expect 100(1-\\alpha) of them to contain the parameter \\mu . We will investigate this using the below density functions and a range of different sample sizes. It is safe to say that the greater the deviation of the population distribution from normality, the larger the sample size needed to achieve the desired confidence level. Here are the results from a simulation where a sample of size n is drawn from each distribution, and a confidence interval is constructed. After constructing the Confidence Interval, we verify whether it contains the parameter \\mu . This process is repeated 1,000 times for each sample size and distribution combination, allowing us to compute the coverage percentage. It's important to note that since this is a simulation, the value of \\mu is known, which would not typically be the case in real-world scenarios. Invoking the Central Limit Theorem (CLT) for smaller samples from non-normal distributions is tricky. As demonstrated in this simulation, the advertised confidence level may not be realized. For instance, with a sample size of n=10 from an exponential distribution, the observed coverage percentage is merely 87\\% though we advertised 95\\% . It is important to recognize that the Central Limit Theorem is rooted in 'Large Sample' theory, meaning the distribution of \\bar{X} is well approximated by a normal distribution for 'large' sample sizes. Below is a helpful visual further investigating the coverage percentage for an exponentially distributed population. \\large{\\textbf{CI for Smaller Sample Sizes}} We observed above that for n=10 , the coverage probability of our CI for an exponentially distributed population was < 90\\% . Please note the following: If X_{1}, X_{2}, ..., X_{n} are iid exponential(\\lambda) , then \\sum X_{i} \\sim Gamma(n, \\lambda) . Thus \\frac{2\\bar{X}}{\\lambda} \\sim Gamma(n, \\frac{2}{n}) is a pivotal quantity since its distribution does not depend on the parameter \\lambda . It follows that 1 - \\alpha = P(a \\leq \\frac{2\\bar{X}}{\\lambda} \\leq b) where a, b are Gamma(n, \\frac{2}{n}) critical values such that P(X < a) = 0.025 and P(X > b) = .025 . A 95\\% CI for \\lambda is therefore 0.95 = P(\\frac{2\\bar{X}}{b} \\leq \\lambda \\leq \\frac{2\\bar{X}}{a}) Below are results using this construction. We demonstrate its reliability primarily for smaller sample sizes, as these are the cases where invoking the Central Limit Theorem is more uncertain. Two things to note: Gamma(\\alpha, \\beta) \\rightarrow N(\\alpha\\beta, \\alpha\\beta^{2}) as \\alpha -> \\infty . In our case, \\alpha is the sample size. This construction may not be too useful in practice since we likely won't be able to reliably judge the distribution of the population from which we sampled. Especially for small sample sizes.","title":"Confidence Intervals"},{"location":"Statistical%20Inference/Section%202%3A%20confidence_intervals/#confidence-intervals","text":"In a previous section , we learned that \\overline{X} is a consistent/trustworthy estimate of \\mu . If n is \"large\", CLT provides that \\bar{X} \\sim N(\\mu, \\frac{\\sigma^{2}}{n}) \\label{clt} \\tag{1} \\textbf{Question} : How can we take advantage of the Central Limit Theorem to learn more about our parameter of interest, \\mu ? Can we construct a range of values that is likely to contain \\mu ? Based on our distribution result, we can standardize \\eqref{clt}: \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1) Here, Z = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} is a standard normal random variable whose distribution does not depend on \\mu or \\sigma . This is referred to as a pivotal quantity. Let P(Z > z_{\\frac{\\alpha}{2}}) = \\frac{\\alpha}{2} and note: \\begin{equation} \\begin{aligned} 1 - \\alpha &= P(-z_{\\frac{\\alpha}{2}} < \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} < z_{\\frac{\\alpha}{2}}) \\\\ & = P(\\overline{X} - z_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}} < \\mu < \\overline{X} + z_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}}) \\\\ \\end{aligned} \\end{equation} \\large{\\textbf{Case I (\u03c3 is known)}} : Let X_{1},...,X_{n} be random sample of size n from a population w/ mean \\mu and \\textit{known} variance \\sigma^{2} . A 100(1-\\alpha)\\% Confidence Interval for \\mu is given by: \\overline{X} \\pm z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} where the value of z_{\\frac{\\alpha}{2}} depends on desired confidence level. Confidence Level Z_{\\frac{\\alpha}{2}} 95% 1.96 90% 1.645 99% 2.576 \\underline{\\textbf{Example}} In a random sample of 75 Rowan students, the mean height was found to be 67 inches. Assuming the population standard deviation is 7 inches, construct a 95% confidence interval for the mean height of all Rowan students. Solution We have \\overline{x} = 67 , \\sigma = 7 , n=75 , z_{\\frac{\\alpha}{2}} = 1.96 . Therefore, a 95% CI for \\mu is: \\begin{equation} \\begin{aligned} & \\hspace{3mm} \\overline{x} \\pm 1.96 \\frac{7}{\\sqrt{75}} \\\\ & = (65.4, 68.58) \\\\ \\end{aligned} \\end{equation} Interpretation: With 95% confidence, we estimate that the mean height of all Rowan students is somewhere between 65.4 and 68.58. Note: from the sample we found \\overline{x} = 67 inches. Using CI, we're saying we're 95% confident that \\mu is somewhere between 65.4 and 68.58. \\textit{Limitation} of Z Confidence Interval: \\sigma is unlikely to be known. \\large{\\textbf{Case I (\u03c3 is unknown)}} : Since \\sigma^{2} in unlikely to be known to the practitioner, we are often limited to s^{2} . The quantity \\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} does not follow a normal distribution. Rather, it follows a t-distribution with n-1 degrees of freedom. For smaller sample sizes, the t-distribution has heavier tails than the normal distribution and thus we expect more extreme values. However, as the degrees of freedom approach infinity, the t-distribution converges to the normal distribution. CI when \u03c3 is unknown Let X_{1}, ..., X_{n} be a random sample from a population with mean \\mu and unknown variance \\sigma^{2} , a 100(1-\\alpha)% CI for \\mu is given by: \\overline{x} \\pm t_{\\frac{\\alpha}{2}, n-1} \\frac{s}{\\sqrt{n}} where t_{\\frac{\\alpha}{2}, n-1} depends on both sample size and confidence level. \\large{\\textbf{CI Coverage}} The correct interpretation of a Confidence Interval is as follows: With 100(1-\\alpha)\\% confidence, we estimate that \\mu is somewhere between L and U. Using the word probability is not correct when it comes to interpreting these intervals. For example, if \\mu = 73 and we calculate a 95\\% confidence interval: (65, 72). There isn't a 95 percent chance that 73 is in the interval! It is a yes/no question. Rather, if we were to calculate 100 different 100(1-\\alpha)\\% confidence intervals for \\mu using 100 different samples of the sample size from the same population, we would expect 100(1-\\alpha) of them to contain the parameter \\mu . We will investigate this using the below density functions and a range of different sample sizes. It is safe to say that the greater the deviation of the population distribution from normality, the larger the sample size needed to achieve the desired confidence level. Here are the results from a simulation where a sample of size n is drawn from each distribution, and a confidence interval is constructed. After constructing the Confidence Interval, we verify whether it contains the parameter \\mu . This process is repeated 1,000 times for each sample size and distribution combination, allowing us to compute the coverage percentage. It's important to note that since this is a simulation, the value of \\mu is known, which would not typically be the case in real-world scenarios. Invoking the Central Limit Theorem (CLT) for smaller samples from non-normal distributions is tricky. As demonstrated in this simulation, the advertised confidence level may not be realized. For instance, with a sample size of n=10 from an exponential distribution, the observed coverage percentage is merely 87\\% though we advertised 95\\% . It is important to recognize that the Central Limit Theorem is rooted in 'Large Sample' theory, meaning the distribution of \\bar{X} is well approximated by a normal distribution for 'large' sample sizes. Below is a helpful visual further investigating the coverage percentage for an exponentially distributed population. \\large{\\textbf{CI for Smaller Sample Sizes}} We observed above that for n=10 , the coverage probability of our CI for an exponentially distributed population was < 90\\% . Please note the following: If X_{1}, X_{2}, ..., X_{n} are iid exponential(\\lambda) , then \\sum X_{i} \\sim Gamma(n, \\lambda) . Thus \\frac{2\\bar{X}}{\\lambda} \\sim Gamma(n, \\frac{2}{n}) is a pivotal quantity since its distribution does not depend on the parameter \\lambda . It follows that 1 - \\alpha = P(a \\leq \\frac{2\\bar{X}}{\\lambda} \\leq b) where a, b are Gamma(n, \\frac{2}{n}) critical values such that P(X < a) = 0.025 and P(X > b) = .025 . A 95\\% CI for \\lambda is therefore 0.95 = P(\\frac{2\\bar{X}}{b} \\leq \\lambda \\leq \\frac{2\\bar{X}}{a}) Below are results using this construction. We demonstrate its reliability primarily for smaller sample sizes, as these are the cases where invoking the Central Limit Theorem is more uncertain. Two things to note: Gamma(\\alpha, \\beta) \\rightarrow N(\\alpha\\beta, \\alpha\\beta^{2}) as \\alpha -> \\infty . In our case, \\alpha is the sample size. This construction may not be too useful in practice since we likely won't be able to reliably judge the distribution of the population from which we sampled. Especially for small sample sizes.","title":"Confidence Intervals"}]}